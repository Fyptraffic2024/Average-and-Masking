{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719cedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "# from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "# from torchvision.transforms import functional as F\n",
    "from ultralytics import YOLO\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea7b8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5dc0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects_from_video(csv_file, video_file, object_id, output_folder):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Filter the rows for the specified object ID\n",
    "    df_object = df[df['ID'] == object_id]\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Loop through the filtered DataFrame\n",
    "    for index, row in df_object.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        x1, y1, x2, y2 = int(row['x1']), int(row['y1']), int(row['x2']), int(row['y2'])\n",
    "        \n",
    "        # Set the video frame to the correct frame number\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if ret:\n",
    "            # Crop the object using the bounding box coordinates\n",
    "            cropped_object = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            # Save the cropped object as an image\n",
    "            output_filename = f\"{output_folder}/object_{object_id}_frame_{frame_number}.jpg\"\n",
    "            cv2.imwrite(output_filename, cropped_object)\n",
    "        else:\n",
    "            print(f\"Could not read frame {frame_number} from the video.\")\n",
    "    \n",
    "    # Release the video capture\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "059ca42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'vid7.csv'\n",
    "video_file = 'E:/Dataset_project/Crop_videos/vid7_27_7_FaisalTown.mp4'\n",
    "object_id = 4  # Specify the object ID you want to extract\n",
    "output_folder = 'E:/Dataset_project/extracted_objects_4'\n",
    "extract_objects_from_video(csv_file, video_file, object_id, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data = pd.read_csv(\"vid7.csv\")\n",
    "# Filter for only ID = 1\n",
    "object_data = data[data['ID'] == 1]\n",
    "# Load the average frame\n",
    "average_frame = cv2.imread(\"E:/Dataset_project/Summarization_videos/average_frame.jpg\")\n",
    "# Define video properties\n",
    "frame_height, frame_width, _ = average_frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('E:/Dataset_project/Track_on_summarize_video/output_video.mp4', fourcc, 30, (frame_width, frame_height))\n",
    "# Iterate over each unique frame number for ID = 1\n",
    "for frame_number in sorted(object_data['Frame'].unique()):\n",
    "    # Copy the average frame to start fresh for each frame\n",
    "    frame = average_frame.copy()\n",
    "    # Get bounding box data for the specific frame\n",
    "    bbox = object_data[object_data['Frame'] == frame_number].iloc[0]\n",
    "    x1, y1, x2, y2 = int(bbox['x1']), int(bbox['y1']), int(bbox['x2']), int(bbox['y2'])\n",
    "    \n",
    "    # Clear the bounding box area in the current frame\n",
    "    frame[y1:y2, x1:x2] = 0\n",
    "    \n",
    "    # Load the extracted object image for this frame\n",
    "    object_img_path = f\"E:/Dataset_project/extracted_objects/object_1_frame_{frame_number}.0.jpg\"\n",
    "    object_img = cv2.imread(object_img_path)\n",
    "    \n",
    "    # Resize object image to fit the bounding box size\n",
    "    object_img_resized = cv2.resize(object_img, (x2 - x1, y2 - y1))\n",
    "    \n",
    "    # Paste the resized object image onto the cleared bounding box area\n",
    "    frame[y1:y2, x1:x2] = object_img_resized\n",
    "    \n",
    "    # Write the frame to the video\n",
    "    output_video.write(frame)\n",
    "\n",
    "# Release the video writer\n",
    "output_video.release()\n",
    "print(\"Video created successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd71d23",
   "metadata": {},
   "source": [
    "### Applying Mask YOLO(Average Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"vid7.csv\")\n",
    "\n",
    "# Filter for only ID = 1\n",
    "object_data = data[data['ID'] == 1]\n",
    "\n",
    "# Load the average frame\n",
    "average_frame = cv2.imread(\"E:/Dataset_project/Summarization_videos/average_frame.jpg\")\n",
    "\n",
    "# Define video properties\n",
    "frame_height, frame_width, _ = average_frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('E:/Dataset_project/Track_on_summarize_video/output_mask_video.mp4', fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "# Iterate over each unique frame number for ID = 1\n",
    "for frame_number in sorted(object_data['Frame'].unique()):\n",
    "    # Copy the average frame to start fresh for each frame\n",
    "    frame = average_frame.copy()\n",
    "    \n",
    "    # Get bounding box data for the specific frame\n",
    "    bbox = object_data[object_data['Frame'] == frame_number].iloc[0]\n",
    "    x1, y1, x2, y2 = int(bbox['x1']), int(bbox['y1']), int(bbox['x2']), int(bbox['y2'])\n",
    "    \n",
    "    # Clear the bounding box area in the current frame\n",
    "    frame[y1:y2, x1:x2] = 0\n",
    "    \n",
    "    # Load the extracted object image for this frame\n",
    "    object_img_path = f\"E:/Dataset_project/extracted_objects/object_1_frame_{frame_number}.0.jpg\"\n",
    "    object_img = cv2.imread(object_img_path)\n",
    "    \n",
    "    # Run YOLOv8 segmentation on the object image\n",
    "    results = model(object_img, conf=0.5)\n",
    "    \n",
    "    # Get the segmentation mask\n",
    "    masks = results[0].masks\n",
    "\n",
    "    if masks is not None and len(masks) > 0:\n",
    "        # Convert the mask to a numpy array and ensure it's in uint8 format\n",
    "        mask = masks.data[0].numpy().astype(np.uint8)  # Get the first mask and convert to uint8\n",
    "\n",
    "        # Resize the binary mask to match the bounding box size\n",
    "        mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Resize object image to fit the bounding box size\n",
    "        object_img_resized = cv2.resize(object_img, (x2 - x1, y2 - y1))\n",
    "\n",
    "        # Ensure mask is binary (0 or 255)\n",
    "        mask_resized = (mask_resized * 255).astype(np.uint8)\n",
    "\n",
    "        # Create the area from the average frame to blend into non-detected areas\n",
    "        background_region = average_frame[y1:y2, x1:x2].copy()  # Extract same-sized region from average frame\n",
    "\n",
    "        # Invert the mask to get non-detected areas as 255 (white)\n",
    "        inverse_mask = cv2.bitwise_not(mask_resized)\n",
    "\n",
    "        # Apply the inverted mask to the background region\n",
    "        non_detected_areas = cv2.bitwise_and(background_region, background_region, mask=inverse_mask)\n",
    "\n",
    "        # Apply segmentation mask to the resized object image\n",
    "        object_img_masked = cv2.bitwise_and(object_img_resized, object_img_resized, mask=mask_resized)\n",
    "\n",
    "        # Combine detected and non-detected areas\n",
    "        object_img_masked = cv2.add(object_img_masked, non_detected_areas)\n",
    "\n",
    "        # Overlay the masked object image with background-based non-detected areas onto the cleared bounding box area\n",
    "        frame[y1:y2, x1:x2] = object_img_masked\n",
    "\n",
    "    # Write the frame to the video\n",
    "    output_video.write(frame)\n",
    "\n",
    "# Release the video writer\n",
    "output_video.release()\n",
    "print(\"Video with segmentation mask created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315689ee",
   "metadata": {},
   "source": [
    "### Applying Mask (Averaging of back and next frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"vid7.csv\")\n",
    "\n",
    "# Filter for only ID = 1\n",
    "object_data = data[data['ID'] == 1]\n",
    "\n",
    "# Load the average frame\n",
    "average_frame = cv2.imread(\"E:/Dataset_project/Summarization_videos/average_frame.jpg\")\n",
    "\n",
    "# Define video properties\n",
    "frame_height, frame_width, _ = average_frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('E:/Dataset_project/Track_on_summarize_video/output_no_black_video.mp4', fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "# Define threshold for zero pixels\n",
    "zero_pixel_threshold = 50  # Adjust as needed\n",
    "\n",
    "# Dictionary to store processed frames and buffer for missing content\n",
    "output_frames = {}\n",
    "last_valid_frame_content = None\n",
    "\n",
    "# Process each frame for the given object ID\n",
    "for frame_number in sorted(object_data['Frame'].unique()):\n",
    "    # Copy the average frame to start fresh for each frame\n",
    "    frame = average_frame.copy()\n",
    "    \n",
    "    # Get bounding box data for the specific frame\n",
    "    bbox = object_data[object_data['Frame'] == frame_number].iloc[0]\n",
    "    x1, y1, x2, y2 = int(bbox['x1']), int(bbox['y1']), int(bbox['x2']), int(bbox['y2'])\n",
    "    \n",
    "    # Clear the bounding box area in the current frame\n",
    "    frame[y1:y2, x1:x2] = 0\n",
    "    \n",
    "    # Load the extracted object image for this frame\n",
    "    object_img_path = f\"E:/Dataset_project/extracted_objects/object_1_frame_{frame_number}.0.jpg\"\n",
    "    object_img = cv2.imread(object_img_path)\n",
    "    \n",
    "    # Run YOLOv8 segmentation on the object image\n",
    "    results = model(object_img, conf=0.5)\n",
    "    \n",
    "    # Get the segmentation mask\n",
    "    masks = results[0].masks\n",
    "\n",
    "    if masks is not None and len(masks) > 0:\n",
    "        # Convert the mask to a numpy array and ensure it's in uint8 format\n",
    "        mask = masks.data[0].numpy().astype(np.uint8)  # Get the first mask and convert to uint8\n",
    "\n",
    "        # Resize the binary mask to match the bounding box size\n",
    "        mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Resize object image to fit the bounding box size\n",
    "        object_img_resized = cv2.resize(object_img, (x2 - x1, y2 - y1))\n",
    "\n",
    "        # Ensure mask is binary (0 or 255)\n",
    "        mask_resized = (mask_resized * 255).astype(np.uint8)\n",
    "\n",
    "        # Create the area from the average frame to blend into non-detected areas\n",
    "        background_region = average_frame[y1:y2, x1:x2].copy()  # Extract same-sized region from average frame\n",
    "\n",
    "        # Invert the mask to get non-detected areas as 255 (white)\n",
    "        inverse_mask = cv2.bitwise_not(mask_resized)\n",
    "\n",
    "        # Apply the inverted mask to the background region\n",
    "        non_detected_areas = cv2.bitwise_and(background_region, background_region, mask=inverse_mask)\n",
    "\n",
    "        # Apply segmentation mask to the resized object image\n",
    "        object_img_masked = cv2.bitwise_and(object_img_resized, object_img_resized, mask=mask_resized)\n",
    "\n",
    "        # Combine detected and non-detected areas\n",
    "        object_img_masked = cv2.add(object_img_masked, non_detected_areas)\n",
    "\n",
    "        # Overlay the masked object image with background-based non-detected areas onto the cleared bounding box area\n",
    "        frame[y1:y2, x1:x2] = object_img_masked\n",
    "\n",
    "        # Update the last valid content with the current bounding box\n",
    "        last_valid_frame_content = frame[y1:y2, x1:x2].copy()\n",
    "    else:\n",
    "        # If there's no detection, check for zero pixels\n",
    "        if last_valid_frame_content is not None:\n",
    "            # Resize last_valid_frame_content to match the bounding box size\n",
    "            resized_content = cv2.resize(last_valid_frame_content, (x2 - x1, y2 - y1))\n",
    "            # Propagate resized content to avoid black areas\n",
    "            frame[y1:y2, x1:x2] = resized_content\n",
    "        else:\n",
    "            # Use background if no previous content is available\n",
    "            frame[y1:y2, x1:x2] = average_frame[y1:y2, x1:x2].copy()\n",
    "\n",
    "    # Save the processed frame in output_frames dictionary\n",
    "    output_frames[frame_number] = frame\n",
    "\n",
    "# Write all frames to the video\n",
    "for frame_number in sorted(output_frames.keys()):\n",
    "    output_video.write(output_frames[frame_number])\n",
    "\n",
    "# Release the video writer\n",
    "output_video.release()\n",
    "print(\"Video with filled gaps created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422b00c",
   "metadata": {},
   "source": [
    "### Mask RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8efcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the CSV data and filter for ID = 1\n",
    "data = pd.read_csv(\"vid7.csv\")\n",
    "object_data = data[data['ID'] == 1]\n",
    "\n",
    "# Load the average frame\n",
    "average_frame = cv2.imread(\"E:/Dataset_project/Summarization_videos/average_frame.jpg\")\n",
    "\n",
    "# Define video properties\n",
    "frame_height, frame_width, _ = average_frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('E:/Dataset_project/Track_on_summarize_video/output_maskRCNN_video.mp4', fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "# Initialize Mask R-CNN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Iterate over each unique frame number for ID = 1\n",
    "total_frames = len(object_data['Frame'].unique())\n",
    "for i, frame_number in enumerate(sorted(object_data['Frame'].unique()), 1):\n",
    "    # Copy the average frame to start fresh for each frame\n",
    "    frame = average_frame.copy()\n",
    "    \n",
    "    # Get bounding box data for the specific frame\n",
    "    bbox = object_data[object_data['Frame'] == frame_number].iloc[0]\n",
    "    x1, y1, x2, y2 = int(bbox['x1']), int(bbox['y1']), int(bbox['x2']), int(bbox['y2'])\n",
    "    \n",
    "    # Clear the bounding box area in the current frame\n",
    "    frame[y1:y2, x1:x2] = 0\n",
    "    \n",
    "    # Load the extracted object image for this frame\n",
    "    object_img_path = f\"E:/Dataset_project/extracted_objects/object_1_frame_{frame_number}.0.jpg\"\n",
    "    object_img = cv2.imread(object_img_path)\n",
    "    \n",
    "    # Prepare the object image for Mask R-CNN input\n",
    "    img_tensor = F.to_tensor(object_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run Mask R-CNN on the object image\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_tensor)\n",
    "\n",
    "    # Filter predictions with a confidence threshold\n",
    "    masks = predictions[0]['masks']\n",
    "    scores = predictions[0]['scores']\n",
    "    confidence_threshold = 0.5\n",
    "    mask = None\n",
    "\n",
    "    # Check if any masks exist above the threshold\n",
    "    for j in range(len(scores)):\n",
    "        if scores[j] > confidence_threshold:\n",
    "            mask = masks[j, 0].cpu().numpy()\n",
    "            break\n",
    "\n",
    "    if mask is not None:\n",
    "        # Resize the binary mask to match the bounding box size\n",
    "        mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Resize object image to fit the bounding box size\n",
    "        object_img_resized = cv2.resize(object_img, (x2 - x1, y2 - y1))\n",
    "\n",
    "        # Ensure mask is binary (0 or 255)\n",
    "        mask_resized = (mask_resized * 255).astype(np.uint8)\n",
    "\n",
    "        # Create the area from the average frame to blend into non-detected areas\n",
    "        background_region = average_frame[y1:y2, x1:x2].copy()\n",
    "\n",
    "        # Invert the mask to get non-detected areas as 255 (white)\n",
    "        inverse_mask = cv2.bitwise_not(mask_resized)\n",
    "\n",
    "        # Apply the inverted mask to the background region\n",
    "        non_detected_areas = cv2.bitwise_and(background_region, background_region, mask=inverse_mask)\n",
    "\n",
    "        # Apply segmentation mask to the resized object image\n",
    "        object_img_masked = cv2.bitwise_and(object_img_resized, object_img_resized, mask=mask_resized)\n",
    "\n",
    "        # Combine detected and non-detected areas\n",
    "        object_img_masked = cv2.add(object_img_masked, non_detected_areas)\n",
    "\n",
    "        # Overlay the masked object image with background-based non-detected areas onto the cleared bounding box area\n",
    "        frame[y1:y2, x1:x2] = object_img_masked\n",
    "\n",
    "    # Write the frame to the video\n",
    "    output_video.write(frame)\n",
    "    # Print progress feedback\n",
    "    sys.stdout.write(f\"\\rProcessing frame {i}/{total_frames}...\")\n",
    "    sys.stdout.flush()\n",
    "# Release the video writer\n",
    "output_video.release()\n",
    "print(\"\\nVideo with Mask R-CNN segmentation mask created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741178ae",
   "metadata": {},
   "source": [
    "### YOLO with Car only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aeb9de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"vid7.csv\")\n",
    "object_data = data[data['ID'] == 1]\n",
    "\n",
    "# Load the average frame\n",
    "average_frame = cv2.imread(\"E:/Dataset_project/Summarization_videos/average_frame.jpg\")\n",
    "\n",
    "# Define video properties\n",
    "frame_height, frame_width, _ = average_frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('E:/Dataset_project/Track_on_summarize_video/output_carmask_video.mp4', fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "# Define the class ID for car in YOLOv8\n",
    "car_class_id = 2  # Update this to the correct ID for \"car\" in your model if different\n",
    "\n",
    "# Iterate over each unique frame number for ID = 1\n",
    "for frame_number in sorted(object_data['Frame'].unique()):\n",
    "    # Copy the average frame to start fresh for each frame\n",
    "    frame = average_frame.copy()\n",
    "    \n",
    "    # Get bounding box data for the specific frame\n",
    "    bbox = object_data[object_data['Frame'] == frame_number].iloc[0]\n",
    "    x1, y1, x2, y2 = int(bbox['x1']), int(bbox['y1']), int(bbox['x2']), int(bbox['y2'])\n",
    "    \n",
    "    # Check if bounding box coordinates are valid\n",
    "    if x2 - x1 <= 0 or y2 - y1 <= 0:\n",
    "        print(f\"Skipping frame {frame_number} due to invalid bounding box dimensions.\")\n",
    "        continue\n",
    "\n",
    "    # Clear the bounding box area in the current frame\n",
    "    frame[y1:y2, x1:x2] = 0\n",
    "    \n",
    "    # Load the extracted object image for this frame\n",
    "    object_img_path = f\"E:/Dataset_project/extracted_objects/object_1_frame_{frame_number}.0.jpg\"\n",
    "    object_img = cv2.imread(object_img_path)\n",
    "    \n",
    "    # Run YOLOv8 segmentation on the object image\n",
    "    results = model(object_img, conf=0.5)\n",
    "\n",
    "    # Get segmentation masks and class IDs from YOLOv8 model output\n",
    "    masks = results[0].masks\n",
    "    boxes = results[0].boxes\n",
    "\n",
    "    # Check if detections exist in this frame\n",
    "    if masks is None or boxes is None:\n",
    "        print(f\"Frame {frame_number}: No detections found, skipping this frame.\")\n",
    "        continue\n",
    "\n",
    "    # Filter to keep only car masks\n",
    "    car_mask = None\n",
    "    for mask, box in zip(masks, boxes):\n",
    "        if int(box.cls) == car_class_id:  # Check if the class ID matches \"car\"\n",
    "            car_mask = mask.data.numpy().astype(np.uint8)\n",
    "            break\n",
    "\n",
    "    # Debug statements to check the dimensions of car_mask and bounding box\n",
    "    if car_mask is not None:\n",
    "        # Ensure car_mask is 2D by taking the first channel if needed\n",
    "        if car_mask.ndim == 3:\n",
    "            car_mask = car_mask[0]\n",
    "\n",
    "        print(f\"Frame {frame_number}: Car mask shape before resizing: {car_mask.shape}\")\n",
    "        print(f\"Frame {frame_number}: Target bounding box dimensions: {(y2 - y1, x2 - x1)}\")\n",
    "\n",
    "        # Resize the binary car mask to match the bounding box size\n",
    "        mask_resized = cv2.resize(car_mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Resize object image to fit the bounding box size\n",
    "        object_img_resized = cv2.resize(object_img, (x2 - x1, y2 - y1))\n",
    "\n",
    "        # Ensure mask is binary (0 or 255)\n",
    "        mask_resized = (mask_resized * 255).astype(np.uint8)\n",
    "\n",
    "        # Create the area from the average frame to blend into non-detected areas\n",
    "        background_region = average_frame[y1:y2, x1:x2].copy()\n",
    "\n",
    "        # Invert the mask to get non-detected areas as 255 (white)\n",
    "        inverse_mask = cv2.bitwise_not(mask_resized)\n",
    "\n",
    "        # Apply the inverted mask to the background region\n",
    "        non_detected_areas = cv2.bitwise_and(background_region, background_region, mask=inverse_mask)\n",
    "\n",
    "        # Apply segmentation mask to the resized object image\n",
    "        object_img_masked = cv2.bitwise_and(object_img_resized, object_img_resized, mask=mask_resized)\n",
    "\n",
    "        # Combine detected and non-detected areas\n",
    "        object_img_masked = cv2.add(object_img_masked, non_detected_areas)\n",
    "\n",
    "        # Overlay the masked object image with background-based non-detected areas onto the cleared bounding box area\n",
    "        frame[y1:y2, x1:x2] = object_img_masked\n",
    "    else:\n",
    "        print(f\"No car mask found for frame {frame_number}. Skipping mask application for this frame.\")\n",
    "\n",
    "    # Write the frame to the video\n",
    "    output_video.write(frame)\n",
    "    print(f\"Processed frame {frame_number}\")  # Indicate progress for each processed frame\n",
    "\n",
    "# Release the video writer\n",
    "output_video.release()\n",
    "print(\"Video with car-only segmentation mask created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7a3c5",
   "metadata": {},
   "source": [
    "### Multiple object placing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bbfb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 1 car, 1821.0ms\n",
      "Speed: 68.0ms preprocess, 1821.0ms inference, 165.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 (no detections), 1275.0ms\n",
      "Speed: 31.0ms preprocess, 1275.0ms inference, 5.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 1 car, 767.0ms\n",
      "Speed: 11.0ms preprocess, 767.0ms inference, 13.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 608x640 (no detections), 932.0ms\n",
      "Speed: 14.0ms preprocess, 932.0ms inference, 5.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "\n",
      "0: 448x640 1 car, 677.1ms\n",
      "Speed: 13.0ms preprocess, 677.1ms inference, 13.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 968.0ms\n",
      "Speed: 15.0ms preprocess, 968.0ms inference, 8.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 739.0ms\n",
      "Speed: 11.0ms preprocess, 739.0ms inference, 12.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 933.0ms\n",
      "Speed: 14.0ms preprocess, 933.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 625.0ms\n",
      "Speed: 7.0ms preprocess, 625.0ms inference, 12.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 955.0ms\n",
      "Speed: 15.0ms preprocess, 955.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 627.0ms\n",
      "Speed: 8.0ms preprocess, 627.0ms inference, 11.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 862.0ms\n",
      "Speed: 13.0ms preprocess, 862.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 1 motorcycle, 733.0ms\n",
      "Speed: 7.0ms preprocess, 733.0ms inference, 20.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 854.0ms\n",
      "Speed: 19.0ms preprocess, 854.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 624.0ms\n",
      "Speed: 8.0ms preprocess, 624.0ms inference, 13.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 869.0ms\n",
      "Speed: 14.0ms preprocess, 869.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 662.0ms\n",
      "Speed: 11.0ms preprocess, 662.0ms inference, 12.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 934.0ms\n",
      "Speed: 13.0ms preprocess, 934.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 1 motorcycle, 862.0ms\n",
      "Speed: 45.0ms preprocess, 862.0ms inference, 16.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 823.0ms\n",
      "Speed: 12.0ms preprocess, 823.0ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 2 motorcycles, 829.0ms\n",
      "Speed: 6.0ms preprocess, 829.0ms inference, 23.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 (no detections), 948.0ms\n",
      "Speed: 22.0ms preprocess, 948.0ms inference, 7.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 448x640 1 car, 1 motorcycle, 695.0ms\n",
      "Speed: 17.0ms preprocess, 695.0ms inference, 24.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x640 (no detections), 1050.0ms\n",
      "Speed: 19.0ms preprocess, 1050.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 2 motorcycles, 726.0ms\n",
      "Speed: 7.0ms preprocess, 726.0ms inference, 21.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 (no detections), 1039.0ms\n",
      "Speed: 15.0ms preprocess, 1039.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 2 motorcycles, 633.0ms\n",
      "Speed: 6.0ms preprocess, 633.0ms inference, 19.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 (no detections), 976.0ms\n",
      "Speed: 12.0ms preprocess, 976.0ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 2 motorcycles, 615.0ms\n",
      "Speed: 7.0ms preprocess, 615.0ms inference, 19.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 640x640 (no detections), 921.0ms\n",
      "Speed: 15.0ms preprocess, 921.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 416x640 1 car, 1 motorcycle, 1234.0ms\n",
      "Speed: 62.0ms preprocess, 1234.0ms inference, 21.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"vid7.csv\")\n",
    "\n",
    "# Define IDs to process and corresponding folders\n",
    "object_ids = [1, 4]  # Update to IDs 1 and 4\n",
    "object_folders = {\n",
    "    1: \"E:/Dataset_project/extracted_objects/\",  # Folder for ID 1\n",
    "    4: \"E:/Dataset_project/extracted_objects_4/\",  # Folder for ID 4\n",
    "}\n",
    "\n",
    "# Load the average frame\n",
    "average_frame = cv2.imread(\"E:/Dataset_project/Summarization_videos/average_frame.jpg\")\n",
    "\n",
    "# Define video properties\n",
    "frame_height, frame_width, _ = average_frame.shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('E:/Dataset_project/Track_on_summarize_video/output_ids_1_and_4.mp4', fourcc, 30, (frame_width, frame_height))\n",
    "\n",
    "# Define threshold for zero pixels\n",
    "zero_pixel_threshold = 50  # Adjust as needed\n",
    "\n",
    "# Dictionary to store processed frames\n",
    "output_frames = {}\n",
    "\n",
    "# Process frames for all IDs\n",
    "for frame_number in sorted(data['Frame'].unique()):\n",
    "    # Start with the average frame for each output frame\n",
    "    combined_frame = average_frame.copy()\n",
    "\n",
    "    for obj_id in object_ids:\n",
    "        # Filter data for the current object ID and frame\n",
    "        object_data = data[(data['ID'] == obj_id) & (data['Frame'] == frame_number)]\n",
    "\n",
    "        # Skip if no data for this ID in the current frame\n",
    "        if object_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Get bounding box data\n",
    "        bbox = object_data.iloc[0]\n",
    "        x1, y1, x2, y2 = int(bbox['x1']), int(bbox['y1']), int(bbox['x2']), int(bbox['y2'])\n",
    "\n",
    "        # Clear the bounding box area\n",
    "        combined_frame[y1:y2, x1:x2] = 0\n",
    "\n",
    "        # Load the extracted object image for this frame\n",
    "        object_img_path = f\"{object_folders[obj_id]}object_{obj_id}_frame_{frame_number}.0.jpg\"\n",
    "        object_img = cv2.imread(object_img_path)\n",
    "\n",
    "        if object_img is None:\n",
    "            continue\n",
    "\n",
    "        # Run YOLOv8 segmentation on the object image\n",
    "        results = model(object_img, conf=0.5)\n",
    "\n",
    "        # Get the segmentation mask\n",
    "        masks = results[0].masks\n",
    "\n",
    "        if masks is not None and len(masks) > 0:\n",
    "            # Convert the mask to a numpy array and ensure it's in uint8 format\n",
    "            mask = masks.data[0].numpy().astype(np.uint8)\n",
    "\n",
    "            # Resize the binary mask and object image to fit the bounding box size\n",
    "            mask_resized = cv2.resize(mask, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "            object_img_resized = cv2.resize(object_img, (x2 - x1, y2 - y1))\n",
    "\n",
    "            # Ensure mask is binary (0 or 255)\n",
    "            mask_resized = (mask_resized * 255).astype(np.uint8)\n",
    "\n",
    "            # Create the area from the average frame to blend into non-detected areas\n",
    "            background_region = average_frame[y1:y2, x1:x2].copy()\n",
    "\n",
    "            # Invert the mask to get non-detected areas\n",
    "            inverse_mask = cv2.bitwise_not(mask_resized)\n",
    "            non_detected_areas = cv2.bitwise_and(background_region, background_region, mask=inverse_mask)\n",
    "\n",
    "            # Apply segmentation mask to the resized object image\n",
    "            object_img_masked = cv2.bitwise_and(object_img_resized, object_img_resized, mask=mask_resized)\n",
    "\n",
    "            # Combine detected and non-detected areas\n",
    "            combined_content = cv2.add(object_img_masked, non_detected_areas)\n",
    "\n",
    "            # Overlay the processed object area onto the combined frame\n",
    "            combined_frame[y1:y2, x1:x2] = combined_content\n",
    "\n",
    "    # Save the processed frame in the output_frames dictionary\n",
    "    output_frames[frame_number] = combined_frame\n",
    "\n",
    "# Write all frames to the video\n",
    "for frame_number in sorted(output_frames.keys()):\n",
    "    output_video.write(output_frames[frame_number])\n",
    "\n",
    "# Release the video writer\n",
    "output_video.release()\n",
    "print(\"Video with IDs 1 and 4 created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
